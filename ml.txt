exp7

import numpy as np
import matplotlib.pyplot as plt

# Generate a sample dataset (replace this with your own dataset)
np.random.seed(42)
exam_scores = np.random.normal(loc=70, scale=10, size=100)

# Create a histogram
plt.hist(exam_scores, bins=20, edgecolor='black', alpha=0.7)
plt.title('Distribution of Exam Scores')
plt.xlabel('Score')
plt.ylabel('Frequency')

# Plot quartiles
q1 = np.percentile(exam_scores, 25)
q2 = np.percentile(exam_scores, 50)
q3 = np.percentile(exam_scores, 75)

plt.axvline(q1, color='r', linestyle='--', label='Q1')
plt.axvline(q2, color='g', linestyle='--', label='Q2 (Median)')
plt.axvline(q3, color='b', linestyle='--', label='Q3')

plt.legend()

# Display the plot
plt.show()

# Analysis
print(f"Q1 (25th percentile): {q1}")
print(f"Q2 (50th percentile - Median): {q2}")
print(f"Q3 (75th percentile): {q3}")

# Check for outliers
iqr = q3 - q1
lower_bound = q1 - 1.5 * iqr
upper_bound = q3 + 1.5 * iqr

outliers = exam_scores[(exam_scores < lower_bound) | (exam_scores > upper_bound)]

if len(outliers) > 0:
    print(f"\nOutliers: {outliers}")
else:
    print("\nNo outliers detected.")


exp8 

import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd

# Load the Iris dataset from seaborn
iris = sns.load_dataset('iris')

# Create distribution chart for sepal length
plt.figure(figsize=(12, 5))

plt.subplot(1, 2, 1)
sns.histplot(iris['sepal_length'], bins=20, kde=True, color='skyblue')
plt.title('Distribution of Sepal Length')
plt.xlabel('Sepal Length')
plt.ylabel('Frequency')

# Create distribution chart for petal length
plt.subplot(1, 2, 2)
sns.histplot(iris['petal_length'], bins=20, kde=True, color='salmon')
plt.title('Distribution of Petal Length')
plt.xlabel('Petal Length')
plt.ylabel('Frequency')

plt.tight_layout()

# Create scatter plot for sepal length vs. petal length
plt.figure(figsize=(8, 6))
sns.scatterplot(x='sepal_length', y='petal_length', data=iris, hue='species', palette='Set2')
plt.title('Scatter Plot of Sepal Length vs. Petal Length')
plt.xlabel('Sepal Length')
plt.ylabel('Petal Length')

# Display the plots
plt.show()

# Observations
print("Observations:")
print("1. Sepal Length Distribution:")
print("   - Sepal length distribution appears to have multiple peaks.")
print("   - The data may suggest the presence of different species in the dataset.")
print("\n2. Petal Length Distribution:")
print("   - Petal length distribution shows clear distinctions between different species.")
print("\n3. Scatter Plot:")
print("   - There is a positive correlation between sepal length and petal length.")
print("   - Different species are distinguishable based on their sepal and petal lengths.")

exp9


import seaborn as sns
import matplotlib.pyplot as plt

# Load the Tips dataset from Seaborn
tips = sns.load_dataset('tips')

# Bubble chart
plt.figure(figsize=(12, 6))
sns.scatterplot(x='total_bill', y='tip', size='size', hue='size', sizes=(20, 200),
                data=tips, alpha=0.7, palette='viridis', edgecolor='w', legend='brief')

plt.title('Bubble Chart: Total Bill vs. Tip with Party Size')
plt.xlabel('Total Bill ($)')
plt.ylabel('Tip ($)')

# Density chart (2D density plot)
plt.figure(figsize=(12, 6))
sns.kdeplot(x='total_bill', y='tip', fill=True, cmap='Blues', levels=5, data=tips)

plt.title('2D Density Plot: Total Bill vs. Tip')
plt.xlabel('Total Bill ($)')
plt.ylabel('Tip ($)')

# Show the plots
plt.show()

# Interpretation
print("Interpretation:")
print("1. Bubble Chart:")
print("   - Bubble sizes represent the party size (number of people) for each meal.")
print("   - Colors indicate different party sizes.")
print("   - The chart helps visualize the relationship between total bill and tip, considering party size.")
print("\n2. Density Chart:")
print("   - The density chart provides insights into the concentration of data points.")
print("   - Darker areas indicate higher data concentration.")
print("   - Patterns in density help identify regions of interest or potential outliers.")



exp10


import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import confusion_matrix, accuracy_score, classification_report


# Generate some sample data (replace this with your dataset)
X = np.random.rand(100, 2)  # Features
y = np.random.randint(0, 2, 100)  # Target labels (binary classification)


# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)


# Create a KNN classifier with k=3
knn = KNeighborsClassifier(n_neighbors=3)


# Fit the classifier to the training data
knn.fit(X_train, y_train)


# Make predictions on the test data
y_pred = knn.predict(X_test)


# Calculate and print the confusion matrix
conf_matrix = confusion_matrix(y_test, y_pred)
print("Confusion Matrix:")
print(conf_matrix)


# Calculate and print the accuracy score
accuracy = accuracy_score(y_test, y_pred)
print("Accuracy:", accuracy)
# Print the classification report (including precision, recall, f1-score, and support)
class_report = classification_report(y_test, y_pred)
print("Classification Report:")
print(class_report)

exp11

# Import necessary libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error

# Step 1: Load the dataset
df = pd.read_csv('house_prices.csv')

# Step 2: Explore the dataset
print("Dataset Overview:")
print(df.head())
print("\nSummary Statistics:")
print(df.describe())
print("\nMissing Values:")
print(df.isnull().sum())

# Step 3: Visualize relationships
sns.pairplot(df, x_vars=['Size', 'Bedrooms', 'Bathrooms'], y_vars='Price', height=4, aspect=1, kind='scatter')
plt.show()

# Step 4: Split the dataset
X = df[['Size', 'Bedrooms', 'Bathrooms']]
y = df['Price']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Step 5-6: Build and train the linear regression model
model = LinearRegression()
model.fit(X_train, y_train)

# Step 7: Evaluate the model
y_pred = model.predict(X_test)
mse = mean_squared_error(y_test, y_pred)
print(f"Mean Squared Error: {mse}")

# Step 8: Visualize the regression line
plt.figure(figsize=(10, 6))

# Scatter plot
plt.scatter(X_test['Size'], y_test, color='blue', label='Actual Prices')
plt.scatter(X_test['Size'], y_pred, color='red', label='Predicted Prices')

# Regression line
plt.plot(X_test['Size'], model.predict(X_test), color='green', linewidth=3)

plt.title('Linear Regression: Predicted vs. Actual Prices')
plt.xlabel('Size')
plt.ylabel('Price')
plt.legend()
plt.show()

# Step 9: Interpret coefficients
coefficients = pd.DataFrame({'Feature': X.columns, 'Coefficient': model.coef_})
print("\nCoefficients:")
print(coefficients)

# Step 10: Explore model improvements (optional)

# Additional analysis, visualization, and interpretation can be performed here.


exp12 linear



# Import necessary libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error

# Step 1: Load the dataset
df = pd.read_csv('house_prices_multiple_features.csv')

# Step 2: Explore the dataset
print("Dataset Overview:")
print(df.head())
print("\nSummary Statistics:")
print(df.describe())
print("\nMissing Values:")
print(df.isnull().sum())

# Step 3: Visualize relationships
sns.pairplot(df, x_vars=['Size', 'Bedrooms', 'Bathrooms', 'Garage', 'Pool'], y_vars='Price', height=4, aspect=1, kind='scatter')
plt.show()

# Step 4: Split the dataset
X = df[['Size', 'Bedrooms', 'Bathrooms', 'Garage', 'Pool']]
y = df['Price']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Step 5-6: Build and train the multiple linear regression model
model = LinearRegression()
model.fit(X_train, y_train)

# Step 7: Evaluate the model
y_pred = model.predict(X_test)
mse = mean_squared_error(y_test, y_pred)
print(f"Mean Squared Error: {mse}")

# Step 8: Interpret coefficients
coefficients = pd.DataFrame({'Feature': X.columns, 'Coefficient': model.coef_})
print("\nCoefficients:")
print(coefficients)

# Step 9: Optionally, explore model improvements (feature engineering, regularization, etc.)

# Additional analysis, visualization, and interpretation can be performed here.


exp 13 decisontree


from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
import matplotlib.pyplot as plt
from sklearn import tree

# Load the Iris dataset
iris = load_iris()
X = iris.data
y = iris.target

# Split the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Create a Decision Tree classifier
clf = DecisionTreeClassifier()
clf.fit(X_train, y_train)

# Make predictions on the test set
y_pred = clf.predict(X_test)

# Calculate the accuracy
accuracy = accuracy_score(y_test, y_pred)
print(f'Accuracy: {accuracy:.2f}')

# Display the classification report and confusion matrix
print(classification_report(y_test, y_pred))
print(f'Confusion Matrix:\n{confusion_matrix(y_test, y_pred)}')

# Plot the decision tree
plt.figure(figsize=(12, 8))
tree.plot_tree(clf, feature_names=iris.feature_names, class_names=iris.target_names, filled=True)
plt.show()

14 kmean

import numpy as np
import matplotlib.pyplot as plt

# Generate sample data
data = np.array([[2, 3], [3, 4], [5, 6], [9, 10], [10, 8], [12, 12]])

# Number of clusters (you can change this)
k = 2

# Initialize centroids randomly
centroids = data[np.random.choice(data.shape[0], k, replace=False)]

# Main K-means algorithm
num_iterations = 100

for iteration in range(num_iterations):
    # Calculate distances between data points and centroids
    distances = np.linalg.norm(data[:, np.newaxis] - centroids, axis=2)
   
    # Assign data points to the nearest centroid
    labels = np.argmin(distances, axis=1)
    print(labels)
   
    # Plot data points and current centroids
    plt.scatter(data[:, 0], data[:, 1], c=labels)
    plt.scatter(centroids[:, 0], centroids[:, 1], c='red', marker='X', s=200)
    plt.xlabel("X-axis")
    plt.ylabel("Y-axis")
    plt.title(f"Iteration {iteration}")
    plt.show()
   
    # Update centroids based on the mean of assigned data points
    new_centroids = np.array([data[labels == i].mean(axis=0) for i in range(k)])
   
    # Check for convergence
    if np.array_equal(centroids, new_centroids):
        break
   
    centroids = new_centroids

# Print final cluster assignments and centroids
print(distances)
print("Final Cluster Assignments:", labels)
print("Final Cluster Centroids:", centroids)

exp 17 webcrawler

import requests
from bs4 import BeautifulSoup

def simple_web_crawler(url, max_depth=2):
    visited_urls = set()

    def crawl(url, depth):
        if depth > max_depth or url in visited_urls:
            return
        print(f"Crawling: {url}")
        try:
            response = requests.get(url)
            visited_urls.add(url)
            if response.status_code == 200:
                soup = BeautifulSoup(response.text, 'html.parser')
                title = soup.title.string.strip() if soup.title else 'No title found'
                print(f"Page Title: {title}")
                for link in soup.find_all('a', href=True):
                    next_url = link['href']
                    crawl(next_url, depth + 1)
        except Exception as e:
            print(f"Error crawling {url}: {e}")
    crawl(url, depth=1)
if __name__ == "__main__":
    start_url = "https://example.com"
    simple_web_crawler(start_url)


18 neural network


import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder
from tensorflow.keras import models, layers

# Load data from a CSV file
data = pd.read_csv('iris.csv')

X = data[['sepal_length', 'sepal_width', 'petal_length', 'petal_width']].values
y = data['species'].values

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

label_encoder = LabelEncoder()
y_train = label_encoder.fit_transform(y_train)
y_test = label_encoder.transform(y_test)

# Create a simple feedforward neural network model
model = models.Sequential()
model.add(layers.Dense(64, activation='relu', input_shape=(4,)))
model.add(layers.Dense(32, activation='relu'))
model.add(layers.Dense(3, activation='softmax'))  # Output layer with 3 classes

# Compile the model
model.compile(optimizer='adam',
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])

# Train the model
model.fit(X_train, y_train, epochs=50, batch_size=8, validation_split=0.1)

# Evaluate the model on the test set
test_loss, test_acc = model.evaluate(X_test, y_test)
print(f'Test accuracy: {test_acc}')

19 feed forward



import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.datasets import load_iris
from tensorflow.keras import models, layers


# Load the Iris dataset
iris = load_iris()
X = iris.data
y = iris.target


# Split the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)


# Standardize the features
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)


# One-hot encode the target variable
label_encoder = LabelEncoder()
y_train = label_encoder.fit_transform(y_train)
y_test = label_encoder.transform(y_test)


# Create a simple feedforward neural network model
model = models.Sequential()
model.add(layers.Dense(64, activation='relu', input_shape=(4,)))
model.add(layers.Dense(32, activation='relu'))
model.add(layers.Dense(3, activation='softmax'))  # Output layer with 3 classes


# Compile the model
model.compile(optimizer='adam',
              loss='sparse_categorical_crossentropy',  # Use 'categorical_crossentropy' if y is one-hot encoded
              metrics=['accuracy'])


# Train the model
model.fit(X_train, y_train, epochs=50, batch_size=8, validation_split=0.1)


# Evaluate the model on the test set
test_loss, test_acc = model.evaluate(X_test, y_test)
print(f'Test accuracy: {test_acc}')


